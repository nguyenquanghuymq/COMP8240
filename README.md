# COMP8240

Novel Project

Exploring the limits of Transfer Learning with a Unified Text-To-Text Transfer Transformer

## Introduction

In this paper, we will explore the replicability of the research "Exploring the limits of Transfer Learning with a Unified Text-To-Text Transfer Transformer". 
In the original paper, the authors have taken the pre-train T5 model and fine-tuned it with much smaller task-specific datasets. The result was very positive in different benchmarks.
We will apply the code to the original data to see if the result in the description is accurate. Then create new datasets for the model to see if the claim(s) made by the original work still holds.


## Original paper:

Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J. Liu


## Tasks to replicate:

The Recognizing Textual Entailment
Sentence Acceptability Judgement
Sentiment Analysis
The Semantic Textual Similarity


